{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Сравнение интересов аудитории телеканалов НТВ и Дождь с помощью тематического моделирования LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Сравнить интересы аудитории телеканолов НТВ и Дождь с помощью методов тематического моделирования\n",
    "1. Получить данные по аудитории из социальной сети ВК\n",
    "2. Зарегистрировать приложение, получить app_id, access_token\n",
    "3. Скачать данные по пользователям в каждой из групп (id групп ВК даны ниже, tvrain_id, ntv_id)\n",
    "4. Взять небольшую выборку из каждой совокупности телезрителей(около 1000-2000 человек, т.к. 300k-400k слишком много), с которыми работать дальше\n",
    "5. Обучить LDA модель на их подписках\n",
    "6. По группам, на которые подписаны эти люди, полуичть ключевые слова групп, на которые они подписаны\n",
    "7. Получить распределение интересов людей для каждой группы, сравнить на графике\n",
    "\n",
    "Дополнительно:\n",
    "- По любой группе пользователей построить матрицу Пользователь-Паблик, где 1 будет означать факт подписки на определенный паблик, 0 - отсутствие подписки (строки - пользователи, столбцы - паблики) и применить PCA - метод главных компонент, посмотреть, что из себя представляют топ-5 компонент и описать, какие интересы пользователей эти компоненты представляют"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import sys  \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для использования VK API необходимо создать приложение в VK\n",
    "\n",
    "1. Создать приложение по адресу https://vk.com/apps?act=manage (кнопка \"создать приложение\")\n",
    "2. При создании указать название, описание (можно любые), категория  - прочее. Тип - standalone приложение\n",
    "3. В настройках получить **app_id**. App_id потребуется для получения access token\n",
    "4. Авторизовать пользователя (получить access token) можно по адресу: https://vk.com/dev/first_guide, в правилах нас интересует пункт 3 **Авторизация пользователя**\n",
    "5. После того, как ознакомитесь с авторизацией пользователя, скопируйте в адресную строку такой запрос https://oauth.vk.com/authorize?client_id=6893269&display=page&redirect_uri=https://oauth.vk.com/blank.html&scope=friends&response_type=token&v=5.52, где число **5490057** замените на число, которое получите для вашего **app_id**\n",
    "6. Нажмите Enter. Откроется окно с запросом прав. В нем отображаются название приложения, иконки прав доступа, и Ваши имя с фамилией. Нажмите «Разрешить». Вы попадете на новую страницу с предупреждением о том, что токен нельзя копировать и передавать третьим лицам. В адресной строке будет URL https://oauth.vk.com/blank.html, а после # Вы увидите дополнительные параметры — access_token, expires_in и user_id. Токен может выглядеть, например, так: 51eff86578a3bbbcb5c7043a122a69fd04dca057ac821dd7afd7c2d8e35b60172d45a26599c08034cc40a\n",
    "7. Токен — это Ваш ключ доступа. При выполнении определенных условий человек, получивший Ваш токен, может нанести существенный ущерб Вашим данным и данным других людей. Поэтому очень важно не передавать свой токен третьим лицам\n",
    "8. Поле expires_in содержит время жизни токена в секундах. 86400 секунд — это ровно сутки. Через сутки полученный токен перестанет действовать, для продолжения работы нужно будет получить новый по такому же алгоритму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own app id and respective tokens\n",
    "\n",
    "# скопируйте сюда ваши app_id и access_token, полученные по методу, описанному выше\n",
    "app_id = 6893269\n",
    "access_token = '5397d14e99ab7346e24030b2cc1b2b58738619c18c18b7d25a221ec35507549c9663785f1e7a2f282c78e'\n",
    "\n",
    "# id групп ВК Дождя и НТВ\n",
    "tvrain_id = 17568841\n",
    "ntv_id = 28658784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лейес Миша\n"
     ]
    }
   ],
   "source": [
    "# проверка работы API и авторизации пользователя. Если возникает ошибка, то, возможно, access token необходимо обновить\n",
    "check_id = 54789\n",
    "\n",
    "# api call and test\n",
    "def vk_get_response(method, parameters, token):\n",
    "    url = 'https://api.vk.com/method/' + method + '?' + parameters + '&access_token=' + token\n",
    "#     print url\n",
    "    return(requests.get(url).json())\n",
    "\n",
    "answer = vk_get_response(\n",
    "    'users.get', 'user_ids={0}&v=4.9&lang=ru'.format(check_id), access_token\n",
    ")['response']\n",
    "print(answer[0]['first_name'], answer[0]['last_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение подписчиков телеканалов НТВ и Дождь в VK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим объекты, которые содержат всю информацию о подпиичиках соответствующих групп (указанных в domains) и сохраним их на диск. Получим в итоге два файла - **ntv_subs** и **tvrain_subs** в формате **.pkl** - питоновский формат хранения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset:  0\n",
      "Offset:  1000\n",
      "Offset:  2000\n",
      "Offset:  3000\n",
      "Offset:  4000\n",
      "Offset:  5000\n",
      "Offset:  6000\n",
      "Offset:  7000\n",
      "Offset:  8000\n",
      "Offset:  9000\n",
      "Offset:  10000\n",
      "Offset:  11000\n",
      "Offset:  12000\n",
      "Offset:  13000\n",
      "Offset:  14000\n",
      "Offset:  15000\n",
      "Offset:  16000\n",
      "Offset:  17000\n",
      "Offset:  18000\n",
      "Offset:  19000\n",
      "Offset:  20000\n",
      "Offset:  21000\n",
      "Offset:  22000\n",
      "Offset:  23000\n",
      "Offset:  24000\n",
      "Offset:  25000\n",
      "Offset:  26000\n",
      "Offset:  27000\n",
      "Offset:  28000\n",
      "Offset:  29000\n",
      "Offset:  30000\n",
      "Offset:  31000\n",
      "Offset:  32000\n",
      "Offset:  33000\n",
      "Offset:  34000\n",
      "Offset:  35000\n",
      "Offset:  36000\n",
      "Offset:  37000\n",
      "Offset:  38000\n",
      "Offset:  39000\n",
      "Offset:  40000\n",
      "Offset:  41000\n",
      "Offset:  42000\n",
      "Offset:  43000\n",
      "Offset:  44000\n",
      "Offset:  45000\n",
      "Offset:  46000\n",
      "Offset:  47000\n",
      "Offset:  48000\n",
      "Offset:  49000\n",
      "Offset:  50000\n",
      "Offset:  51000\n",
      "Offset:  52000\n",
      "Offset:  53000\n",
      "Offset:  54000\n",
      "Offset:  55000\n",
      "Offset:  56000\n",
      "Offset:  57000\n",
      "Offset:  58000\n",
      "Offset:  59000\n",
      "Offset:  60000\n",
      "Offset:  61000\n",
      "Offset:  62000\n",
      "Offset:  63000\n",
      "Offset:  64000\n",
      "Offset:  65000\n",
      "Offset:  66000\n",
      "Offset:  67000\n",
      "Offset:  68000\n",
      "Offset:  69000\n",
      "Offset:  70000\n",
      "Offset:  71000\n",
      "Offset:  72000\n",
      "Offset:  73000\n",
      "Offset:  74000\n",
      "Offset:  75000\n",
      "Offset:  76000\n",
      "Offset:  77000\n",
      "Offset:  78000\n",
      "Offset:  79000\n",
      "Offset:  80000\n",
      "Offset:  81000\n",
      "Offset:  82000\n",
      "Offset:  83000\n",
      "Offset:  84000\n",
      "Offset:  85000\n",
      "Offset:  86000\n",
      "Offset:  87000\n",
      "Offset:  88000\n",
      "Offset:  89000\n",
      "Offset:  90000\n",
      "Offset:  91000\n",
      "Offset:  92000\n",
      "Offset:  93000\n",
      "Offset:  94000\n",
      "Offset:  95000\n",
      "Offset:  96000\n",
      "Offset:  97000\n",
      "Offset:  98000\n",
      "Offset:  99000\n",
      "Offset:  100000\n",
      "Offset:  101000\n",
      "Offset:  102000\n",
      "Offset:  103000\n",
      "Offset:  104000\n",
      "Offset:  105000\n",
      "Offset:  106000\n",
      "Offset:  107000\n",
      "Offset:  108000\n",
      "Offset:  109000\n",
      "Offset:  110000\n",
      "Offset:  111000\n",
      "Offset:  112000\n",
      "Offset:  113000\n",
      "Offset:  114000\n",
      "Offset:  115000\n",
      "Offset:  116000\n",
      "Offset:  117000\n",
      "Offset:  118000\n",
      "Offset:  119000\n",
      "Offset:  120000\n",
      "Offset:  121000\n",
      "Offset:  122000\n",
      "Offset:  123000\n",
      "Offset:  124000\n",
      "Offset:  125000\n",
      "Offset:  126000\n",
      "Offset:  127000\n",
      "Offset:  128000\n",
      "Offset:  129000\n",
      "Offset:  130000\n",
      "Offset:  131000\n",
      "Offset:  132000\n",
      "Offset:  133000\n",
      "Offset:  134000\n",
      "Offset:  135000\n",
      "Offset:  136000\n",
      "Offset:  137000\n",
      "Offset:  138000\n",
      "Offset:  139000\n",
      "Offset:  140000\n",
      "Offset:  141000\n",
      "Offset:  142000\n",
      "Offset:  143000\n",
      "Offset:  144000\n",
      "Offset:  145000\n",
      "Offset:  146000\n",
      "Offset:  147000\n",
      "Offset:  148000\n",
      "Offset:  149000\n",
      "Offset:  150000\n",
      "Offset:  151000\n",
      "Offset:  152000\n",
      "Offset:  153000\n",
      "Offset:  154000\n",
      "Offset:  155000\n",
      "Offset:  156000\n",
      "Offset:  157000\n",
      "Offset:  158000\n",
      "Offset:  159000\n",
      "Offset:  160000\n",
      "Offset:  161000\n",
      "Offset:  162000\n",
      "Offset:  163000\n",
      "Offset:  164000\n",
      "Offset:  165000\n",
      "Offset:  166000\n",
      "Offset:  167000\n",
      "Offset:  168000\n",
      "Offset:  169000\n",
      "Offset:  170000\n",
      "Offset:  171000\n",
      "Offset:  172000\n",
      "Offset:  173000\n",
      "Offset:  174000\n",
      "Offset:  175000\n",
      "Offset:  176000\n",
      "Offset:  177000\n",
      "Offset:  178000\n",
      "Offset:  179000\n",
      "Offset:  180000\n",
      "Offset:  181000\n",
      "Offset:  182000\n",
      "Offset:  183000\n",
      "Offset:  184000\n",
      "Offset:  185000\n",
      "Offset:  186000\n",
      "Offset:  187000\n",
      "Offset:  188000\n",
      "Offset:  189000\n",
      "Offset:  190000\n",
      "Offset:  191000\n",
      "Offset:  192000\n",
      "Offset:  193000\n",
      "Offset:  194000\n",
      "Offset:  195000\n",
      "Offset:  196000\n",
      "Offset:  197000\n",
      "Offset:  198000\n",
      "Offset:  199000\n",
      "Offset:  200000\n",
      "Offset:  201000\n",
      "Offset:  202000\n",
      "Offset:  203000\n",
      "Offset:  204000\n",
      "Offset:  205000\n",
      "Offset:  206000\n",
      "Offset:  207000\n",
      "Offset:  208000\n",
      "Offset:  209000\n",
      "Offset:  210000\n",
      "Offset:  211000\n",
      "Offset:  212000\n",
      "Offset:  213000\n",
      "Offset:  214000\n",
      "Offset:  215000\n",
      "Offset:  216000\n",
      "Offset:  217000\n",
      "Offset:  218000\n",
      "Offset:  219000\n",
      "Offset:  220000\n",
      "Offset:  221000\n",
      "Offset:  222000\n",
      "Offset:  223000\n",
      "Offset:  224000\n",
      "Offset:  225000\n",
      "Offset:  226000\n",
      "Offset:  227000\n",
      "Offset:  228000\n",
      "Offset:  229000\n",
      "Offset:  230000\n",
      "Offset:  231000\n",
      "Offset:  232000\n",
      "Offset:  233000\n",
      "Offset:  234000\n",
      "Offset:  235000\n",
      "Offset:  236000\n",
      "Offset:  237000\n",
      "Offset:  238000\n",
      "Offset:  239000\n",
      "Offset:  240000\n",
      "Offset:  241000\n",
      "Offset:  242000\n",
      "Offset:  243000\n",
      "Offset:  244000\n",
      "Offset:  245000\n",
      "Offset:  246000\n",
      "Offset:  247000\n",
      "Offset:  248000\n",
      "Offset:  249000\n",
      "Offset:  250000\n",
      "Offset:  251000\n",
      "Offset:  252000\n",
      "Offset:  253000\n",
      "Offset:  254000\n",
      "Offset:  255000\n",
      "Offset:  256000\n",
      "Offset:  257000\n",
      "Offset:  258000\n",
      "Offset:  259000\n",
      "Offset:  260000\n",
      "Offset:  261000\n",
      "Offset:  262000\n",
      "Offset:  263000\n",
      "Offset:  264000\n",
      "Offset:  265000\n",
      "Offset:  266000\n",
      "Offset:  267000\n",
      "Offset:  268000\n",
      "Offset:  269000\n",
      "Offset:  270000\n",
      "Offset:  271000\n",
      "Offset:  272000\n",
      "Offset:  273000\n",
      "Offset:  274000\n",
      "Offset:  275000\n",
      "Offset:  276000\n",
      "Offset:  277000\n",
      "Offset:  278000\n",
      "Offset:  279000\n",
      "Offset:  280000\n",
      "Offset:  281000\n",
      "Offset:  282000\n",
      "Offset:  283000\n",
      "Offset:  284000\n",
      "Offset:  285000\n",
      "Offset:  286000\n",
      "Offset:  287000\n",
      "Offset:  288000\n",
      "Offset:  289000\n",
      "Offset:  290000\n",
      "Offset:  291000\n",
      "Offset:  292000\n",
      "Offset:  293000\n",
      "Offset:  294000\n",
      "Offset:  295000\n",
      "Offset:  296000\n",
      "Offset:  297000\n",
      "Offset:  298000\n",
      "Offset:  299000\n",
      "Offset:  300000\n",
      "Offset:  301000\n",
      "Offset:  302000\n",
      "Offset:  303000\n",
      "Offset:  304000\n",
      "Offset:  305000\n",
      "Offset:  306000\n",
      "Offset:  307000\n",
      "Offset:  308000\n",
      "Offset:  309000\n",
      "Offset:  310000\n",
      "Offset:  311000\n",
      "Offset:  312000\n",
      "Offset:  313000\n",
      "Offset:  314000\n",
      "Offset:  315000\n",
      "Offset:  316000\n",
      "Offset:  317000\n",
      "Offset:  318000\n",
      "Offset:  319000\n",
      "Offset:  320000\n",
      "Offset:  321000\n",
      "Offset:  322000\n",
      "Offset:  323000\n",
      "Offset:  324000\n",
      "Offset:  325000\n",
      "Offset:  326000\n",
      "Offset:  327000\n",
      "Offset:  328000\n",
      "Offset:  329000\n",
      "Offset:  330000\n",
      "Offset:  331000\n",
      "Offset:  332000\n",
      "Offset:  333000\n",
      "Offset:  334000\n",
      "Offset:  335000\n",
      "Offset:  336000\n",
      "Offset:  337000\n",
      "Offset:  338000\n",
      "Offset:  339000\n",
      "Offset:  340000\n",
      "Offset:  341000\n",
      "Offset:  342000\n",
      "Offset:  343000\n",
      "Offset:  344000\n",
      "Offset:  345000\n",
      "Offset:  346000\n",
      "Offset:  347000\n",
      "Offset:  348000\n",
      "Offset:  349000\n",
      "Offset:  350000\n",
      "Offset:  351000\n",
      "Offset:  352000\n",
      "Offset:  353000\n",
      "Offset:  354000\n",
      "Offset:  355000\n",
      "Offset:  356000\n",
      "Offset:  357000\n",
      "Offset:  358000\n",
      "Offset:  359000\n",
      "Offset:  360000\n",
      "Offset:  361000\n",
      "Offset:  362000\n",
      "Offset:  363000\n",
      "Offset:  364000\n",
      "Offset:  365000\n",
      "Offset:  366000\n",
      "Offset:  367000\n",
      "Offset:  368000\n",
      "Offset:  369000\n",
      "Offset:  370000\n",
      "Offset:  0\n",
      "Offset:  1000\n",
      "Offset:  2000\n",
      "Offset:  3000\n",
      "Offset:  4000\n",
      "Offset:  5000\n",
      "Offset:  6000\n",
      "Offset:  7000\n",
      "Offset:  8000\n",
      "Offset:  9000\n",
      "Offset:  10000\n",
      "Offset:  11000\n",
      "Offset:  12000\n",
      "Offset:  13000\n",
      "Offset:  14000\n",
      "Offset:  15000\n",
      "Offset:  16000\n",
      "Offset:  17000\n",
      "Offset:  18000\n",
      "Offset:  19000\n",
      "Offset:  20000\n",
      "Offset:  21000\n",
      "Offset:  22000\n",
      "Offset:  23000\n",
      "Offset:  24000\n",
      "Offset:  25000\n",
      "Offset:  26000\n",
      "Offset:  27000\n",
      "Offset:  28000\n",
      "Offset:  29000\n",
      "Offset:  30000\n",
      "Offset:  31000\n",
      "Offset:  32000\n",
      "Offset:  33000\n",
      "Offset:  34000\n",
      "Offset:  35000\n",
      "Offset:  36000\n",
      "Offset:  37000\n",
      "Offset:  38000\n",
      "Offset:  39000\n",
      "Offset:  40000\n",
      "Offset:  41000\n",
      "Offset:  42000\n",
      "Offset:  43000\n",
      "Offset:  44000\n",
      "Offset:  45000\n",
      "Offset:  46000\n",
      "Offset:  47000\n",
      "Offset:  48000\n",
      "Offset:  49000\n",
      "Offset:  50000\n",
      "Offset:  51000\n",
      "Offset:  52000\n",
      "Offset:  53000\n",
      "Offset:  54000\n",
      "Offset:  55000\n",
      "Offset:  56000\n",
      "Offset:  57000\n",
      "Offset:  58000\n",
      "Offset:  59000\n",
      "Offset:  60000\n",
      "Offset:  61000\n",
      "Offset:  62000\n",
      "Offset:  63000\n",
      "Offset:  64000\n",
      "Offset:  65000\n",
      "Offset:  66000\n",
      "Offset:  67000\n",
      "Offset:  68000\n",
      "Offset:  69000\n",
      "Offset:  70000\n",
      "Offset:  71000\n",
      "Offset:  72000\n",
      "Offset:  73000\n",
      "Offset:  74000\n",
      "Offset:  75000\n",
      "Offset:  76000\n",
      "Offset:  77000\n",
      "Offset:  78000\n",
      "Offset:  79000\n",
      "Offset:  80000\n",
      "Offset:  81000\n",
      "Offset:  82000\n",
      "Offset:  83000\n",
      "Offset:  84000\n",
      "Offset:  85000\n",
      "Offset:  86000\n",
      "Offset:  87000\n",
      "Offset:  88000\n",
      "Offset:  89000\n",
      "Offset:  90000\n",
      "Offset:  91000\n",
      "Offset:  92000\n",
      "Offset:  93000\n",
      "Offset:  94000\n",
      "Offset:  95000\n",
      "Offset:  96000\n",
      "Offset:  97000\n",
      "Offset:  98000\n",
      "Offset:  99000\n",
      "Offset:  100000\n",
      "Offset:  101000\n",
      "Offset:  102000\n",
      "Offset:  103000\n",
      "Offset:  104000\n",
      "Offset:  105000\n",
      "Offset:  106000\n",
      "Offset:  107000\n",
      "Offset:  108000\n",
      "Offset:  109000\n",
      "Offset:  110000\n",
      "Offset:  111000\n",
      "Offset:  112000\n",
      "Offset:  113000\n",
      "Offset:  114000\n",
      "Offset:  115000\n",
      "Offset:  116000\n",
      "Offset:  117000\n",
      "Offset:  118000\n",
      "Offset:  119000\n",
      "Offset:  120000\n",
      "Offset:  121000\n",
      "Offset:  122000\n",
      "Offset:  123000\n",
      "Offset:  124000\n",
      "Offset:  125000\n",
      "Offset:  126000\n",
      "Offset:  127000\n",
      "Offset:  128000\n",
      "Offset:  129000\n",
      "Offset:  130000\n",
      "Offset:  131000\n",
      "Offset:  132000\n",
      "Offset:  133000\n",
      "Offset:  134000\n",
      "Offset:  135000\n",
      "Offset:  136000\n",
      "Offset:  137000\n",
      "Offset:  138000\n",
      "Offset:  139000\n",
      "Offset:  140000\n",
      "Offset:  141000\n",
      "Offset:  142000\n",
      "Offset:  143000\n",
      "Offset:  144000\n",
      "Offset:  145000\n",
      "Offset:  146000\n",
      "Offset:  147000\n",
      "Offset:  148000\n",
      "Offset:  149000\n",
      "Offset:  150000\n",
      "Offset:  151000\n",
      "Offset:  152000\n",
      "Offset:  153000\n",
      "Offset:  154000\n",
      "Offset:  155000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset:  156000\n",
      "Offset:  157000\n",
      "Offset:  158000\n",
      "Offset:  159000\n",
      "Offset:  160000\n",
      "Offset:  161000\n",
      "Offset:  162000\n",
      "Offset:  163000\n",
      "Offset:  164000\n",
      "Offset:  165000\n",
      "Offset:  166000\n",
      "Offset:  167000\n",
      "Offset:  168000\n",
      "Offset:  169000\n",
      "Offset:  170000\n",
      "Offset:  171000\n",
      "Offset:  172000\n",
      "Offset:  173000\n",
      "Offset:  174000\n",
      "Offset:  175000\n",
      "Offset:  176000\n",
      "Offset:  177000\n",
      "Offset:  178000\n",
      "Offset:  179000\n",
      "Offset:  180000\n",
      "Offset:  181000\n",
      "Offset:  182000\n",
      "Offset:  183000\n",
      "Offset:  184000\n",
      "Offset:  185000\n",
      "Offset:  186000\n",
      "Offset:  187000\n",
      "Offset:  188000\n",
      "Offset:  189000\n",
      "Offset:  190000\n",
      "Offset:  191000\n",
      "Offset:  192000\n",
      "Offset:  193000\n",
      "Offset:  194000\n",
      "Offset:  195000\n",
      "Offset:  196000\n",
      "Offset:  197000\n",
      "Offset:  198000\n",
      "Offset:  199000\n",
      "Offset:  200000\n",
      "Offset:  201000\n",
      "Offset:  202000\n",
      "Offset:  203000\n",
      "Offset:  204000\n",
      "Offset:  205000\n",
      "Offset:  206000\n",
      "Offset:  207000\n",
      "Offset:  208000\n",
      "Offset:  209000\n",
      "Offset:  210000\n",
      "Offset:  211000\n",
      "Offset:  212000\n",
      "Offset:  213000\n",
      "Offset:  214000\n",
      "Offset:  215000\n",
      "Offset:  216000\n",
      "Offset:  217000\n",
      "Offset:  218000\n",
      "Offset:  219000\n",
      "Offset:  220000\n",
      "Offset:  221000\n",
      "Offset:  222000\n",
      "Offset:  223000\n",
      "Offset:  224000\n",
      "Offset:  225000\n",
      "Offset:  226000\n",
      "Offset:  227000\n",
      "Offset:  228000\n",
      "Offset:  229000\n",
      "Offset:  230000\n",
      "Offset:  231000\n",
      "Offset:  232000\n",
      "Offset:  233000\n",
      "Offset:  234000\n",
      "Offset:  235000\n",
      "Offset:  236000\n",
      "Offset:  237000\n",
      "Offset:  238000\n",
      "Offset:  239000\n",
      "Offset:  240000\n",
      "Offset:  241000\n",
      "Offset:  242000\n",
      "Offset:  243000\n",
      "Offset:  244000\n",
      "Offset:  245000\n",
      "Offset:  246000\n",
      "Offset:  247000\n",
      "Offset:  248000\n",
      "Offset:  249000\n",
      "Offset:  250000\n",
      "Offset:  251000\n",
      "Offset:  252000\n",
      "Offset:  253000\n",
      "Offset:  254000\n",
      "Offset:  255000\n",
      "Offset:  256000\n",
      "Offset:  257000\n",
      "Offset:  258000\n",
      "Offset:  259000\n",
      "Offset:  260000\n",
      "Offset:  261000\n",
      "Offset:  262000\n",
      "Offset:  263000\n",
      "Offset:  264000\n",
      "Offset:  265000\n",
      "Offset:  266000\n",
      "Offset:  267000\n",
      "Offset:  268000\n",
      "Offset:  269000\n",
      "Offset:  270000\n",
      "Offset:  271000\n",
      "Offset:  272000\n",
      "Offset:  273000\n",
      "Offset:  274000\n",
      "Offset:  275000\n",
      "Offset:  276000\n",
      "Offset:  277000\n",
      "Offset:  278000\n",
      "Offset:  279000\n",
      "Offset:  280000\n",
      "Offset:  281000\n",
      "Offset:  282000\n",
      "Offset:  283000\n",
      "Offset:  284000\n",
      "Offset:  285000\n",
      "Offset:  286000\n",
      "Offset:  287000\n",
      "Offset:  288000\n",
      "Offset:  289000\n",
      "Offset:  290000\n",
      "Offset:  291000\n",
      "Offset:  292000\n",
      "Offset:  293000\n",
      "Offset:  294000\n",
      "Offset:  295000\n",
      "Offset:  296000\n",
      "Offset:  297000\n",
      "Offset:  298000\n",
      "Offset:  299000\n",
      "Offset:  300000\n",
      "Offset:  301000\n",
      "Offset:  302000\n",
      "Offset:  303000\n",
      "Offset:  304000\n",
      "Offset:  305000\n",
      "Offset:  306000\n",
      "Offset:  307000\n",
      "Offset:  308000\n",
      "Offset:  309000\n",
      "Offset:  310000\n",
      "Offset:  311000\n",
      "Offset:  312000\n",
      "Offset:  313000\n",
      "Offset:  314000\n",
      "Offset:  315000\n",
      "Offset:  316000\n",
      "Offset:  317000\n",
      "Offset:  318000\n",
      "Offset:  319000\n",
      "Offset:  320000\n",
      "Offset:  321000\n",
      "Offset:  322000\n",
      "Offset:  323000\n",
      "Offset:  324000\n",
      "Offset:  325000\n",
      "Offset:  326000\n",
      "Offset:  327000\n",
      "Offset:  328000\n",
      "Offset:  329000\n",
      "Offset:  330000\n",
      "Offset:  331000\n",
      "Offset:  332000\n",
      "Offset:  333000\n",
      "Offset:  334000\n",
      "Offset:  335000\n",
      "Offset:  336000\n",
      "Offset:  337000\n",
      "Offset:  338000\n",
      "Offset:  339000\n",
      "Offset:  340000\n",
      "Offset:  341000\n",
      "Offset:  342000\n",
      "Offset:  343000\n",
      "Offset:  344000\n",
      "Offset:  345000\n",
      "Offset:  346000\n",
      "Offset:  347000\n",
      "Offset:  348000\n",
      "Offset:  349000\n",
      "Offset:  350000\n",
      "Offset:  351000\n",
      "Offset:  352000\n",
      "Offset:  353000\n",
      "Offset:  354000\n",
      "Offset:  355000\n",
      "Offset:  356000\n",
      "Offset:  357000\n",
      "Offset:  358000\n",
      "Offset:  359000\n",
      "Offset:  360000\n",
      "Offset:  361000\n",
      "Offset:  362000\n",
      "Offset:  363000\n",
      "Offset:  364000\n",
      "Offset:  365000\n",
      "Offset:  366000\n",
      "Offset:  367000\n",
      "Offset:  368000\n",
      "Offset:  369000\n",
      "Offset:  370000\n",
      "Offset:  371000\n",
      "Offset:  372000\n",
      "Offset:  373000\n",
      "Offset:  374000\n",
      "Offset:  375000\n",
      "Offset:  376000\n",
      "Offset:  377000\n",
      "Offset:  378000\n",
      "Offset:  379000\n",
      "Offset:  380000\n",
      "Offset:  381000\n",
      "Offset:  382000\n",
      "Offset:  383000\n",
      "Offset:  384000\n",
      "Offset:  385000\n",
      "Offset:  386000\n",
      "Offset:  387000\n",
      "Offset:  388000\n",
      "Offset:  389000\n",
      "Offset:  390000\n",
      "Offset:  391000\n",
      "Offset:  392000\n",
      "Offset:  393000\n",
      "Offset:  394000\n",
      "Offset:  395000\n",
      "Offset:  396000\n",
      "Offset:  397000\n",
      "Offset:  398000\n",
      "Offset:  399000\n",
      "Offset:  400000\n",
      "Offset:  401000\n",
      "Offset:  402000\n",
      "Offset:  403000\n",
      "Offset:  404000\n",
      "Offset:  405000\n",
      "Offset:  406000\n",
      "Offset:  407000\n",
      "Offset:  408000\n",
      "Offset:  409000\n",
      "Offset:  410000\n",
      "Offset:  411000\n",
      "Offset:  412000\n",
      "Offset:  413000\n",
      "Offset:  414000\n",
      "Offset:  415000\n",
      "Offset:  416000\n",
      "Offset:  417000\n",
      "Offset:  418000\n",
      "Offset:  419000\n",
      "Offset:  420000\n",
      "Offset:  421000\n",
      "Offset:  422000\n",
      "Offset:  423000\n",
      "Offset:  424000\n",
      "Offset:  425000\n",
      "Offset:  426000\n",
      "Offset:  427000\n",
      "Offset:  428000\n",
      "Offset:  429000\n",
      "Offset:  430000\n",
      "Offset:  431000\n",
      "Offset:  432000\n",
      "Offset:  433000\n",
      "Offset:  434000\n",
      "Offset:  435000\n",
      "Offset:  436000\n",
      "Offset:  437000\n",
      "Offset:  438000\n",
      "Offset:  439000\n",
      "Offset:  440000\n",
      "Offset:  441000\n",
      "Offset:  442000\n",
      "Offset:  443000\n",
      "Offset:  444000\n"
     ]
    }
   ],
   "source": [
    "domains = ['ntv', 'tvrain']\n",
    "\n",
    "\n",
    "for group_domain in domains:\n",
    "    offset = 0\n",
    "    group_id = group_domain\n",
    "    fields = \"\"\"sex,bdate,city,country,home_town,lists,domain,has_mobile,\n",
    "    contacts,connections,education,universities,followers_count,occupation,last_seen,relation\"\"\"\n",
    "    first_sample = vk_get_response(\n",
    "        'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "            group_id, offset, fields\n",
    "        ), token=access_token\n",
    "    )\n",
    "    community_count = first_sample['response']['count']\n",
    "    community_members = []\n",
    "    for i in range(community_count // 1000 + 1):\n",
    "        offset = i * 1000\n",
    "        try:\n",
    "            answer = vk_get_response(\n",
    "                'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "                    group_id, offset, fields), token=access_token\n",
    "            )\n",
    "            print(\"Offset: \", offset)\n",
    "        except:\n",
    "            print(\"Offset: \", offset, \" Error\")\n",
    "        community_members += answer['response']['users']\n",
    "    save_obj(community_members, '{}_subs'.format(group_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv = load_obj('ntv_subs')\n",
    "community_tvrain = load_obj('tvrain_subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv_df = pd.DataFrame(community_ntv)\n",
    "community_tvrain_df = pd.DataFrame(community_tvrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала получим всех уникальных подписчиков НТВ и Дождя с помощью unique. Далее с помощью numpy.random необходимо выбрать небольшой sample (например, по 1000 из каждой группы) таких людей и объединить их вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntv_uids = community_ntv_df.uid.unique().tolist()\n",
    "tvrain_uids = community_tvrain_df.uid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить общий список людей из двух выборок НТВ и Дождя, всего должно быть в итоге около 2000 человек\n",
    "uids = ntv_uids + tvrain_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 profiles done\n",
      "100 profiles done\n",
      "200 profiles done\n",
      "300 profiles done\n",
      "400 profiles done\n",
      "500 profiles done\n",
      "600 profiles done\n",
      "700 profiles done\n",
      "800 profiles done\n",
      "900 profiles done\n"
     ]
    }
   ],
   "source": [
    "# получить подписки этих пользователей\n",
    "print_counter = 0\n",
    "final_data = []\n",
    "\n",
    "for uid in uids[:1000]:\n",
    "    try:\n",
    "        user_subs = vk_get_response(\n",
    "            'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "        )\n",
    "        time.sleep(0.3)\n",
    "        final_data.append(user_subs)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    if print_counter % 100 == 0:\n",
    "        print(\"{0} profiles done\".format(print_counter))\n",
    "    print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 users\n",
      "Processed 0 users\n",
      "Processed 100 users\n",
      "Processed 100 users\n",
      "Processed 200 users\n",
      "Processed 300 users\n",
      "Processed 400 users\n",
      "Processed 400 users\n",
      "Processed 500 users\n",
      "Processed 600 users\n"
     ]
    }
   ],
   "source": [
    "subs_list = []\n",
    "groups_freq_dict = {}\n",
    "top_n = 5\n",
    "\n",
    "for record, uid in zip(final_data, uids):\n",
    "    try:\n",
    "        user_subs = record\n",
    "        if not user_subs.get('response'):\n",
    "            user_subs = vk_get_response(\n",
    "                'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "            )\n",
    "        subs_pd = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    'groups_count': user_subs['response']['groups'].get('count'),\n",
    "                    'groups_list': user_subs['response']['groups'].get('items'),\n",
    "                    'follows_count':user_subs['response']['users'].get('count'),\n",
    "                    'follows_list': user_subs['response']['users'].get('items'),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for group_id in user_subs['response']['groups'].get('items')[:top_n]:\n",
    "            if groups_freq_dict.get(group_id):\n",
    "                groups_freq_dict[group_id] += 1\n",
    "            else:\n",
    "                groups_freq_dict[group_id] = 1\n",
    "\n",
    "        subs_pd['subs_count'] = subs_pd['groups_count'] + subs_pd['follows_count']\n",
    "        subs_list.append(subs_pd)\n",
    "    except:\n",
    "#         print(user_subs)\n",
    "        pass\n",
    "    if len(subs_list) % 100 == 0:\n",
    "        print(\"Processed {0} users\".format(len(subs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые популярные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68471405, 77),\n",
       " (28658784, 70),\n",
       " (38290762, 18),\n",
       " (18901857, 18),\n",
       " (29686754, 17)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(key, val) for key, val in groups_freq_dict.items()], key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка постов со стен групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 groups extracted\n",
      "200 groups extracted\n",
      "300 groups extracted\n",
      "400 groups extracted\n",
      "500 groups extracted\n",
      "600 groups extracted\n",
      "700 groups extracted\n",
      "Response error. Group id 163822605\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-163822605'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "800 groups extracted\n",
      "900 groups extracted\n",
      "1000 groups extracted\n",
      "1100 groups extracted\n",
      "1200 groups extracted\n",
      "1300 groups extracted\n",
      "1400 groups extracted\n",
      "1500 groups extracted\n",
      "1600 groups extracted\n",
      "1700 groups extracted\n",
      "1800 groups extracted\n",
      "Response error. Group id 28181075\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-28181075'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "1900 groups extracted\n",
      "2000 groups extracted\n",
      "2100 groups extracted\n",
      "2200 groups extracted\n"
     ]
    }
   ],
   "source": [
    "group_doc_dict = {}\n",
    "counter = 0\n",
    "groups_freq_dict_top5 = groups_freq_dict\n",
    "\n",
    "for group_id, freq in groups_freq_dict_top5.items():\n",
    "    counter += 1\n",
    "    try:\n",
    "        check = vk_get_response(\n",
    "            'wall.get',\n",
    "            'owner_id={0}&count=100&fields=post_type,marked_as_ads&&v=4.9&lang=ru'.format(int(group_id) * -1),\n",
    "            access_token\n",
    "        )\n",
    "        check = check['response']\n",
    "        group_doc = ''\n",
    "        if check[0] != 0:\n",
    "            for post in check[1:]:\n",
    "                if post.get('marked_as_ads') != 1:\n",
    "                    group_doc += post['text']\n",
    "        group_doc_dict[group_id] = group_doc\n",
    "    except:\n",
    "        print(\"Response error. Group id {0}\".format(group_id))\n",
    "        print(check)\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} groups extracted\".format(counter))\n",
    "    time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить сырые данные по постам групп на диск\n",
    "save_obj(group_doc_dict, 'group_doc_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\semen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\semen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs_to_delete = string.punctuation + u'»' + u'«' + u'—' + u'“' + u'„' + u'•' + u'#'\n",
    "translation_table = {ord(c): None for c in chrs_to_delete if c != u'-'}\n",
    "units = MorphAnalyzer.DEFAULT_UNITS\n",
    "morph = MorphAnalyzer(result_type=None, units=units)\n",
    "PortSt = PorterStemmer()\n",
    "stopw = set(\n",
    "    [w for w in stopwords.words(['russian', 'english'])]\n",
    "    + [u'это', u'году', u'года', u'также', u'етот',\n",
    "       u'которые', u'который', u'которая', u'поэтому',\n",
    "       u'весь', u'свой', u'мочь', u'eтот', u'например',\n",
    "       u'какой-то', u'кто-то', u'самый', u'очень', u'несколько',\n",
    "       u'источник', u'стать', u'время', u'пока', u'однако',\n",
    "       u'около', u'немного', u'кроме', u'гораздо', u'каждый',\n",
    "       u'первый', u'вполне', u'из-за', u'из-под',\n",
    "       u'второй', u'нужно', u'нужный', u'просто', u'большой',\n",
    "       u'хороший', u'хотеть', u'начать', u'должный', u'новый', u'день',\n",
    "       u'метр', u'получить', u'далее', u'именно', u'апрель',\n",
    "       u'сообщать', u'разный', u'говорить', u'делать',\n",
    "       u'появиться', u'2016',\n",
    "       u'2015', u'получить', u'иметь', u'составить', u'дать', u'читать',\n",
    "       u'ничто', u'достаточно', u'использовать',\n",
    "       u'принять', u'практически',\n",
    "       u'находиться', u'месяц', u'достаточно', u'что-то', u'часто',\n",
    "       u'хотеть', u'начаться', u'делать', u'событие', u'составлять',\n",
    "       u'остаться', u'заявить', u'сделать', u'дело',\n",
    "       u'примерно', u'попасть', u'хотя', u'лишь', u'первое',\n",
    "       u'больший', u'решить', u'число', u'идти', u'давать', u'вопрос',\n",
    "       u'сегодня', u'часть', u'высокий', u'главный', u'случай', u'место',\n",
    "       u'конец', u'работать', u'работа', u'слово', u'важный', u'сказать']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_start = 'http[s]?://'\n",
    "url_end = (\n",
    "    '(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "pattern = url_start + url_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка слов постов групп - трансформация в \"хороший\" вид. Нормализация и стэмминг, удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 docs processed\n",
      "200 docs processed\n",
      "300 docs processed\n",
      "400 docs processed\n",
      "500 docs processed\n",
      "600 docs processed\n",
      "700 docs processed\n",
      "800 docs processed\n",
      "900 docs processed\n",
      "1000 docs processed\n",
      "1100 docs processed\n",
      "1200 docs processed\n",
      "1300 docs processed\n",
      "1400 docs processed\n",
      "1500 docs processed\n",
      "1600 docs processed\n",
      "1700 docs processed\n",
      "1800 docs processed\n",
      "1900 docs processed\n",
      "2000 docs processed\n",
      "2100 docs processed\n",
      "2200 docs processed\n"
     ]
    }
   ],
   "source": [
    "group_clean_doc_dict = {}\n",
    "counter = 0\n",
    "\n",
    "for group_id, doc in group_doc_dict.items():\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    body = ' '.join(\n",
    "        [tag.string.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "         for tag in soup.descendants if tag.string]\n",
    "    )\n",
    "    body = re.sub('\\[.*?\\]','', body)\n",
    "    body = re.sub(pattern,'', body)\n",
    "    if body != '':\n",
    "        body_clean = body.translate(translation_table).lower().strip()\n",
    "        words = word_tokenize(body_clean)\n",
    "        tokens = []\n",
    "        # stemming and text normalization\n",
    "        for word in words:\n",
    "            if re.match('^[a-z0-9-]+$', word) is not None:\n",
    "                tokens.append(PortSt.stem(word))\n",
    "            elif word.count('-') > 1:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                normal_forms = morph.normal_forms(word)\n",
    "                tokens.append(normal_forms[0] if normal_forms else word)\n",
    "        # remove stopwords and leave unique words only\n",
    "        tokens = filter(\n",
    "            lambda token: token not in stopw, sorted(set(tokens))\n",
    "        )\n",
    "\n",
    "        # remove all words with more than 3 chars\n",
    "        tokens = filter(lambda token: len(token) > 3, tokens)\n",
    "    else:\n",
    "        tokens = []\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} docs processed\".format(counter))\n",
    "    group_clean_doc_dict[group_id] = tokens\n",
    "\n",
    "group_clean_doc_dict = {key: list(val) for key, val in group_clean_doc_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить обработанные данные на диск\n",
    "save_obj(group_clean_doc_dict, 'group_doc_dict_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение LDA модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import TextCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "class ListTextCorpus(TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        for doc in self.input:\n",
    "            yield doc\n",
    "                \n",
    "mycorp = ListTextCorpus(input=group_clean_doc_dict.values())\n",
    "justlda = LdaModel(\n",
    "    corpus=mycorp, num_topics=20, passes=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/08/c9/58bbe33a6a440f7ba58b90d5682525ee8d446d288018bbfca0ac2b69f9b0/gensim-3.7.1-cp37-cp37m-win_amd64.whl (24.1MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.15.4)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/7b/d32e81fcf13e96dd679936138d843a40c33163e98454defc485c71b92766/boto3-1.9.111-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.111 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/58/dc/a1e204bd358e45d8e921a70882d5641eaf8890335dd80a210488076fc4dc/botocore-1.12.111-py2.py3-none-any.whl (5.3MB)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\semen\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open: started\n",
      "  Running setup.py bdist_wheel for smart-open: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\semen\\AppData\\Local\\pip\\Cache\\wheels\\f7\\a6\\ff\\9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\semen\\AppData\\Local\\pip\\Cache\\wheels\\81\\75\\d6\\e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.111 botocore-1.12.111 bz2file-0.98 gensim-3.7.1 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel performance\n",
      "0 человек март группа новое город ждать друг найти жизнь знать\n",
      "1 мартакрасивый шокинаcassandra medinavanessa bountyxenia морозоваучастница veronikaучастница macherhammermarcia kaltwasserbrittani зайцевамария романоваучастница\n",
      "2 такаякак 😻как красивокак твой любитель ножка умничек зaкaзaть кapтинк экpaн\n",
      "3 горн ☀всё dream street woman natur nois pink water tram\n",
      "4 клинский годус победыбыть телефонеsiemensдорогий всехвсех свадьбауже хооооий жевсё 90советский роставсех\n",
      "5 рина малахитовый измайлов новосёлов genechkadjoganкатерин jarzabekjanet сканцева sturmela васильеваanita неприятельsusan\n",
      "6 ингредиент масло рецепт сливочный приготовление перемешать соус вкусный растительный нарезать\n",
      "7 любить жизнь любовь твой рука жить думать женщина человек знать\n",
      "8 выcoкoть takoгo зoвyт пpoфeccие тoвapискать кpeдить диктyть ycтaлa дoжить paзy\n",
      "9 убийство чашка щенок самоубийство наполниться 😃😘😘💋🤣🤣🤣🤣🤣🤣😘💋😍🤣😝💗🤣😍👏🏻😝🤣🤣🤣😘👏🏻🤣😋💗😍😦😘💋🤣🤣🤣💗😝🤣1 стол👏🏻😘👏🏻😘😍😦😘👏🏻💋😍💗😘😘🤣😄😲💗😄💋💋👏🏻😲😘💗❤❤❤😲💗👏🏻ухваахах😍 😦👏🏻🤣😍😘💋🤣💋💗😂🤣😲😂😘😦🤣😦💗👏🏻😂🤣помним👏🏻👏🏻🤣😍💋😘😘👏🏻👏🏻😦как поздравимбыть воркаут\n",
      "10 😍крутой 😍марго кидман галь ждeт штейн ❤знать 😎кто заворотнюк гадот😍максимум\n",
      "11 прoстой заpячь доконец колдовстволёд мимикиawaytak спичкойбез солнышкедобрый никакиначеатмосфера традицияхдолжный погодедарить\n",
      "12 жизнии жизнис еёзабавный зодиакачеловек хвалят😍 зодиакавот зодиакальный знатьгороскоп планым зодиакарейтинг\n",
      "13 правильнона дняня фиксиковы темена удобнокто поймутсложный поймутбыть логичнолегендаолд ответвсем поймётво\n",
      "14 плaнировaние cбивать категориикaк понятноc друзьяй тыпpь недocягаемоетeпeрить paботаеткoгда забратьд драматизироватькoгда\n",
      "15 шрифтовой ступенчатый типографик middl предназначить подшипник оборот ковш realiti сварка\n",
      "16 cнизить иcтoчник ocoбeннocть знaчитeльнo кacaeтcть итoгo чиcтoть пpeдoтвpaтить пocлeднeм нaдoлгo\n",
      "17 сeкунда мыcль клубняк рингтон челoвекoм hous playlist music зашкаливать ремикс\n",
      "18 процессор самовывоз аккумулятор личка насадка iphon лспродать детский 42-44 площадь\n",
      "19 раскупать сегодня😉 слушайкачать петербургкосмонавт сингла🚀 москваred авторизоваться 1999гть 2001гть небитый\n"
     ]
    }
   ],
   "source": [
    "print('LdaModel performance')\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group distribution by the most relevant topic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     0.81\n",
       "7     0.15\n",
       "6     0.03\n",
       "17    0.00\n",
       "9     0.00\n",
       "18    0.00\n",
       "2     0.00\n",
       "15    0.00\n",
       "13    0.00\n",
       "1     0.00\n",
       "11    0.00\n",
       "3     0.00\n",
       "10    0.00\n",
       "5     0.00\n",
       "16    0.00\n",
       "14    0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dict = {key: 0 for key in range(20)}\n",
    "\n",
    "group_topics_dict_20 = {\n",
    "    group_id: dict(list(dummy_dict.items()) + justlda.get_document_topics(mycorp.dictionary.doc2bow(text)))\n",
    "    for group_id, text in group_clean_doc_dict.items()\n",
    "}\n",
    "check_pd_20 = pd.DataFrame.from_dict(group_topics_dict_20, orient='index')\n",
    "check_pd_20.head(10)\n",
    "print(\"Group distribution by the most relevant topic\")\n",
    "pd.Series.round(check_pd_20.idxmax(axis=1).value_counts() * 1. / len(check_pd_20), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump lda model to disk\n",
    "justlda.save('ldamodel_20_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most typical groups for every topic\n",
      "0 человек март группа новое город ждать друг найти жизнь знать\n",
      "Цифровая история http://vk.com/club144904445\n",
      "ВПО http://vk.com/club66283435\n",
      "События Петербурга http://vk.com/club106987913\n",
      "США — спонсор  мирового террора (16+) http://vk.com/club38121615\n",
      "Новости Саратова - СарБК http://vk.com/club34841235\n",
      "Голос Турции | Voice of Turkey | Türkiyenin Sesi http://vk.com/club46362087\n",
      "Политика Путина http://vk.com/club23977193\n",
      "Веселый поселок - Невский район http://vk.com/club84147\n",
      "ОТКРЫТОЕ Колпино http://vk.com/club13254334\n",
      "Вежливые Люди http://vk.com/club69628799\n",
      "\n",
      "1 мартакрасивый шокинаcassandra medinavanessa bountyxenia морозоваучастница veronikaучастница macherhammermarcia kaltwasserbrittani зайцевамария романоваучастница\n",
      "SFAP  |  Спортивные девушки http://vk.com/club33390409\n",
      "Лекторий http://vk.com/club95871811\n",
      "MILOTA http://vk.com/club128216930\n",
      "Короче говоря http://vk.com/club144495454\n",
      "SVOИ http://vk.com/club43802\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "Сеть фотоателье - \"ФотоDок\" - СПб - Фотопечать http://vk.com/club86550045\n",
      "ОТВАЛИ, ВАЛЕРА http://vk.com/club127229831\n",
      "qwerty http://vk.com/club127511675\n",
      "Pikabu [GIF | Видео] http://vk.com/club143099415\n",
      "\n",
      "2 такаякак 😻как красивокак твой любитель ножка умничек зaкaзaть кapтинк экpaн\n",
      "TIGER - Мужской журнал http://vk.com/club39144813\n",
      "Компания «Гриф-Медиа» http://vk.com/club35979329\n",
      "HackWhore / ЯНИТАКАЯ http://vk.com/club39080597\n",
      "Deep House http://vk.com/club55526638\n",
      "Мужской рай http://vk.com/club38290762\n",
      "Академия Выдающихся Парней👑 http://vk.com/club24698811\n",
      "※Любители Лысых Ежей※ http://vk.com/club147949920\n",
      "WEB-H.ru http://vk.com/club163427798\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "\n",
      "3 горн ☀всё dream street woman natur nois pink water tram\n",
      "lo fi photo http://vk.com/club159870047\n",
      "Мужской Журнал http://vk.com/club54683761\n",
      "Реальный Дно http://vk.com/club161237483\n",
      "Точка G http://vk.com/club123181602\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "qwerty http://vk.com/club127511675\n",
      "Сумерки http://vk.com/club37056240\n",
      "Деньги в SMM | Блог специалиста по трафику http://vk.com/club114383278\n",
      "S.P.Q.R. http://vk.com/club933556\n",
      "ПСД (Превосходные Славянские Девушки) Фото/видео http://vk.com/club58690425\n",
      "\n",
      "4 клинский годус победыбыть телефонеsiemensдорогий всехвсех свадьбауже хооооий жевсё 90советский роставсех\n",
      "ЛИХИЕ 90Е http://vk.com/club1322986\n",
      "Смейся до слёз :D http://vk.com/club26419239\n",
      "Сеть фотоателье - \"ФотоDок\" - СПб - Фотопечать http://vk.com/club86550045\n",
      "S.P.Q.R. http://vk.com/club933556\n",
      "Деньги в SMM | Блог специалиста по трафику http://vk.com/club114383278\n",
      "X-Potential http://vk.com/club19670451\n",
      "SVOИ http://vk.com/club43802\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "ОТВАЛИ, ВАЛЕРА http://vk.com/club127229831\n",
      "\n",
      "5 рина малахитовый измайлов новосёлов genechkadjoganкатерин jarzabekjanet сканцева sturmela васильеваanita неприятельsusan\n",
      "Чудесные иллюстрации http://vk.com/club76931775\n",
      "X-Potential http://vk.com/club19670451\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "Деньги в SMM | Блог специалиста по трафику http://vk.com/club114383278\n",
      "ПСД (Превосходные Славянские Девушки) Фото/видео http://vk.com/club58690425\n",
      "S.P.Q.R. http://vk.com/club933556\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "Коротко обо мне http://vk.com/club94947512\n",
      "\n",
      "6 ингредиент масло рецепт сливочный приготовление перемешать соус вкусный растительный нарезать\n",
      "Лучшие рецепты Повара http://vk.com/club18464856\n",
      "Рецепты тортов http://vk.com/club59980050\n",
      "Десертомания http://vk.com/club103051133\n",
      "Кулинарное искусство http://vk.com/club47118092\n",
      "Торты  рецепты http://vk.com/club44705173\n",
      "Рецепты - ПростоПовар http://vk.com/club154419301\n",
      "Cook Good - лучшие рецепты http://vk.com/club39009769\n",
      "Готовим дома: вкусно и просто http://vk.com/club40020627\n",
      "Здоровое питание | Правильные Рецепты http://vk.com/club67339983\n",
      "Хозяюшка http://vk.com/club62866871\n",
      "\n",
      "7 любить жизнь любовь твой рука жить думать женщина человек знать\n",
      "MY WORLD.....(18+) http://vk.com/club97511498\n",
      "Вечно Молодая http://vk.com/club170791232\n",
      "Душа давно забытого поэта. http://vk.com/club27771365\n",
      "Факты о родившихся 7 сентября http://vk.com/club127730641\n",
      "Дамский коллектив http://vk.com/club175511021\n",
      "Скорпион  ( 10 марта ) http://vk.com/club26567649\n",
      "Философия | Психология | Саморазвитие http://vk.com/club33080201\n",
      "о,великие женщины http://vk.com/club24486838\n",
      "Когда хочу - тогда и дура! http://vk.com/club162474259\n",
      "Молодая мама http://vk.com/club68518743\n",
      "\n",
      "8 выcoкoть takoгo зoвyт пpoфeccие тoвapискать кpeдить диктyть ycтaлa дoжить paзy\n",
      "Дизайн интерьера http://vk.com/club30276695\n",
      "1З http://vk.com/club64151692\n",
      "Лепрозорий http://vk.com/club65960786\n",
      "Леонардо Дай Винчик http://vk.com/club91050183\n",
      "Не поверишь! http://vk.com/club28477986\n",
      "I ❤️ART http://vk.com/club23626127\n",
      "Книги http://vk.com/club44054326\n",
      "Лепра http://vk.com/club30022666\n",
      "КОМПОТ — Журнал со вкусом http://vk.com/club32615394\n",
      "ЗИ - ЗЛОЙ ИЖЕВЧАНИН | ИЖЕВСК http://vk.com/club101515078\n",
      "\n",
      "9 убийство чашка щенок самоубийство наполниться 😃😘😘💋🤣🤣🤣🤣🤣🤣😘💋😍🤣😝💗🤣😍👏🏻😝🤣🤣🤣😘👏🏻🤣😋💗😍😦😘💋🤣🤣🤣💗😝🤣1 стол👏🏻😘👏🏻😘😍😦😘👏🏻💋😍💗😘😘🤣😄😲💗😄💋💋👏🏻😲😘💗❤❤❤😲💗👏🏻ухваахах😍 😦👏🏻🤣😍😘💋🤣💋💗😂🤣😲😂😘😦🤣😦💗👏🏻😂🤣помним👏🏻👏🏻🤣😍💋😘😘👏🏻👏🏻😦как поздравимбыть воркаут\n",
      "Краткие факты http://vk.com/club75149440\n",
      "Улётный юмор http://vk.com/club145826241\n",
      "КАМЕДИ БОТ | Comedy Club | КВН | Приколы | Юмор http://vk.com/club24373626\n",
      "ALPHA | ЕДИНОБОРСТВА | MMA | UFC | УЛИЧНЫЕ ДРАКИ http://vk.com/club149719449\n",
      "СТУДЕНТ-ЮРИСТ http://vk.com/club31213187\n",
      "Коротко обо мне http://vk.com/club94947512\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "WEB-H.ru http://vk.com/club163427798\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "\n",
      "10 😍крутой 😍марго кидман галь ждeт штейн ❤знать 😎кто заворотнюк гадот😍максимум\n",
      "Комедии http://vk.com/club49266632\n",
      "Girls Art Photos http://vk.com/club90797929\n",
      "Алкоголик http://vk.com/club33098150\n",
      "Квартирный Вопрос http://vk.com/club42541008\n",
      "Фотографии http://vk.com/club23044992\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "Ненормальные фотки http://vk.com/club109129979\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "WEB-H.ru http://vk.com/club163427798\n",
      "\n",
      "11 прoстой заpячь доконец колдовстволёд мимикиawaytak спичкойбез солнышкедобрый никакиначеатмосфера традицияхдолжный погодедарить\n",
      "±  minimalistique http://vk.com/club23190991\n",
      "Селфи Струги Красные http://vk.com/club94388436\n",
      "ЮТАЧ | ВЕСНА http://vk.com/club133777217\n",
      "Дизайн комнаты, квартиры http://vk.com/club36416575\n",
      "Сеть фотоателье - \"ФотоDок\" - СПб - Фотопечать http://vk.com/club86550045\n",
      "X-Potential http://vk.com/club19670451\n",
      "Коротко обо мне http://vk.com/club94947512\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "\n",
      "12 жизнии жизнис еёзабавный зодиакачеловек хвалят😍 зодиакавот зодиакальный знатьгороскоп планым зодиакарейтинг\n",
      "Козероги • http://vk.com/club40055679\n",
      "Рыбы • http://vk.com/club40056206\n",
      "Женская одежда  \"Golden style\"  опт и розница http://vk.com/club165335872\n",
      "※Любители Лысых Ежей※ http://vk.com/club147949920\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "Фотографии http://vk.com/club23044992\n",
      "Ненормальные фотки http://vk.com/club109129979\n",
      "SVOИ http://vk.com/club43802\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "WEB-H.ru http://vk.com/club163427798\n",
      "\n",
      "13 правильнона дняня фиксиковы темена удобнокто поймутсложный поймутбыть логичнолегендаолд ответвсем поймётво\n",
      "Чёткие приколы http://vk.com/club31836774\n",
      "BMW http://vk.com/club36792820\n",
      "Subaru Forester http://vk.com/club53082898\n",
      "ФОТОМОДЕЛИ 1024 PX http://vk.com/club63634621\n",
      "※Любители Лысых Ежей※ http://vk.com/club147949920\n",
      "Короче говоря http://vk.com/club144495454\n",
      "Фотографии http://vk.com/club23044992\n",
      "Ненормальные фотки http://vk.com/club109129979\n",
      "SVOИ http://vk.com/club43802\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "\n",
      "14 плaнировaние cбивать категориикaк понятноc друзьяй тыпpь недocягаемоетeпeрить paботаеткoгда забратьд драматизироватькoгда\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Грубо? Простите. http://vk.com/club24390680\n",
      "Мужской рай http://vk.com/club38290762\n",
      "ПСД (Превосходные Славянские Девушки) Фото/видео http://vk.com/club58690425\n",
      "Деньги в SMM | Блог специалиста по трафику http://vk.com/club114383278\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "Pikabu [GIF | Видео] http://vk.com/club143099415\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "X-Potential http://vk.com/club19670451\n",
      "SVOИ http://vk.com/club43802\n",
      "qwerty http://vk.com/club127511675\n",
      "\n",
      "15 шрифтовой ступенчатый типографик middl предназначить подшипник оборот ковш realiti сварка\n",
      "Just English http://vk.com/club44759043\n",
      "Инструменты | AliExpress http://vk.com/club118696305\n",
      "ТриганДэрия http://vk.com/club177147694\n",
      "ОСТРОУМНЫЕ http://vk.com/club41807730\n",
      "Продажа авто в СПб/Выкуп Автомобилей/ Запчасти http://vk.com/club59970928\n",
      "Intelligent design http://vk.com/club35684707\n",
      "Десигн http://vk.com/club51016572\n",
      "Magazines, books, newspapers in English, IELTS http://vk.com/club48413872\n",
      "АЛИгатор http://vk.com/club144383049\n",
      "Авторынок Крым (Симферополь, Севастополь) http://vk.com/club118663849\n",
      "\n",
      "16 cнизить иcтoчник ocoбeннocть знaчитeльнo кacaeтcть итoгo чиcтoть пpeдoтвpaтить пocлeднeм нaдoлгo\n",
      "Короче, Википедия http://vk.com/club70409438\n",
      "Любовный Гороскоп http://vk.com/club30318830\n",
      "Сумерки http://vk.com/club37056240\n",
      "Короче говоря http://vk.com/club144495454\n",
      "X-Potential http://vk.com/club19670451\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "▲ Чёрный юмор http://vk.com/club29508356\n",
      "Женская одежда  \"Golden style\"  опт и розница http://vk.com/club165335872\n",
      "Сеть фотоателье - \"ФотоDок\" - СПб - Фотопечать http://vk.com/club86550045\n",
      "\n",
      "17 сeкунда мыcль клубняк рингтон челoвекoм hous playlist music зашкаливать ремикс\n",
      "NM | Новая и Лучшая Музыка 2019 http://vk.com/club65465753\n",
      "Ценители женской красоты | 18+ http://vk.com/club30861883\n",
      "Be Fucking Awesome http://vk.com/club41274911\n",
      "Онлайн тв http://vk.com/club34016599\n",
      "EROTIC ★ STYLE | эротика http://vk.com/club56367249\n",
      "Эротика http://vk.com/club43605395\n",
      "vixxkei http://vk.com/club169507023\n",
      "Я фотошоплю как бог http://vk.com/club75338985\n",
      "qwerty http://vk.com/club127511675\n",
      "Сумерки http://vk.com/club37056240\n",
      "\n",
      "18 процессор самовывоз аккумулятор личка насадка iphon лспродать детский 42-44 площадь\n",
      "Отдам даром! Московский район http://vk.com/club93878084\n",
      "Все до 2000 СПБ Санкт-Петербург Питер http://vk.com/club134490415\n",
      "Мода петербургского метро http://vk.com/club80093334\n",
      "Бесплатно за репост. Конкурсы http://vk.com/club97758272\n",
      "Ремонт iphone http://vk.com/club163469870\n",
      "AUTO GID http://vk.com/club163406855\n",
      "Guns http://vk.com/club74641828\n",
      "Фильмы http://vk.com/club40335272\n",
      "Новинки кино http://vk.com/club56176916\n",
      "Женские секреты http://vk.com/club28528027\n",
      "\n",
      "19 раскупать сегодня😉 слушайкачать петербургкосмонавт сингла🚀 москваred авторизоваться 1999гть 2001гть небитый\n",
      "Циник http://vk.com/club26090632\n",
      "Pikabu [GIF | Видео] http://vk.com/club143099415\n",
      "qwerty http://vk.com/club127511675\n",
      "ПСД (Превосходные Славянские Девушки) Фото/видео http://vk.com/club58690425\n",
      "ШУТНИК http://vk.com/club65010959\n",
      "Деньги в SMM | Блог специалиста по трафику http://vk.com/club114383278\n",
      "S.P.Q.R. http://vk.com/club933556\n",
      "Короче говоря http://vk.com/club144495454\n",
      "Члены, хуи, стояк +18 (частное фото и видео) http://vk.com/club62241093\n",
      "SVOИ http://vk.com/club43802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The most typical groups for every topic\")\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))\n",
    "    typical_groups = check_pd_20[i].sort_values(ascending=False).index[:10]\n",
    "    for g in typical_groups:\n",
    "        group_info = vk_get_response(\n",
    "            'groups.getById', 'group_ids={0}&v=4.9&lang=ru'.format(g), access_token\n",
    "        )\n",
    "        print(group_info['response'][0]['name'] + ' ' + 'http://vk.com/club' + str(g))\n",
    "        time.sleep(0.3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_sum_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-812b1b7f9bb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# посчитаем разницу между участием пользователей из разных групп\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# и построим график участия в темах этих групп (НТВ и TVRAIN)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_sum_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_sum_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uids_only_ntv'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata_sum_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uids_only_tvrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata_sum_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'delta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uids_only_ntv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'uids_only_tvrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_sum_count' is not defined"
     ]
    }
   ],
   "source": [
    "# используем все группы VK\n",
    "\n",
    "\n",
    "# посчитаем разницу между участием пользователей из разных групп\n",
    "# и построим график участия в темах этих групп (НТВ и TVRAIN)\n",
    "data_sum_count['delta'] = data_sum_count['uids_only_ntv'] - data_sum_count['uids_only_tvrain']\n",
    "data_sum_count.sort_values(by='delta')[['uids_only_ntv','uids_only_tvrain']].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим график участия в темах этих групп (НТВ, TVRAIN и пользователей из обейх групп)\n",
    "data_sum_count.sort_values(by='delta')[['uids_only_ntv','uids_only_tvrain','uids_ntv_and_tvrain']].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
